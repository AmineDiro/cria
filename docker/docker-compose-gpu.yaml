version: "3.8"

services:
  cria:
    build:
      context: ../
      dockerfile: Dockerfile-gpu
    ports:
      - 3000:3000
    volumes:
      # Specify you Llama model path here
      - ${CRIA_MODEL_PATH}:/app/model.bin
    environment:
      - CRIA_SERVICE_NAME=${CRIA_SERVICE_NAME}
      - CRIA_HOST=${CRIA_HOST}
      - CRIA_PORT=${CRIA_PORT}
      - CRIA_USE_GPU=${CRIA_USE_GPU}
      - CRIA_GPU_LAYERS=${CRIA_GPU_LAYERS}
      - CRIA_ZIPKIN_ENDPOINT=${CRIA_ZIPKIN_ENDPOINT}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

  zipkin:
    container_name: zipkin-server
    image: openzipkin/zipkin
    ports:
      - "9411:9411"
